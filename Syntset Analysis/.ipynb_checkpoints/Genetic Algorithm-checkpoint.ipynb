{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f5c1211-213e-4451-bc30-4b72e848d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90269335-7a7f-4be3-b1e0-f753992ea18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3015b516-2cd9-4e7e-b385-5efc9238f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t25    \n",
      "2  \t19    \n",
      "3  \t27    \n",
      "4  \t31    \n",
      "5  \t34    \n",
      "6  \t32    \n",
      "7  \t37    \n",
      "8  \t31    \n",
      "9  \t31    \n",
      "10 \t32    \n",
      "Selected Features from Original dataset using Logistic: ['general csf', 'brainstem', 'thalamus', 'putamen+pallidum', 'total intracranial', 'left cerebral white matter', 'left cerebral cortex', 'left cerebellum cortex', 'left putamen', 'left pallidum', '4th ventricle', 'left hippocampus', 'csf', 'right cerebral white matter', 'right cerebral cortex', 'right lateral ventricle', 'right inferior lateral ventricle', 'right caudate', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-caudalmiddlefrontal', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-medialorbitofrontal', 'ctx-lh-paracentral', 'ctx-lh-parstriangularis', 'ctx-lh-pericalcarine', 'ctx-lh-postcentral', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-superiorparietal', 'ctx-lh-superiortemporal', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-cuneus', 'ctx-rh-inferiorparietal', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-parahippocampal', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-parstriangularis', 'ctx-rh-posteriorcingulate', 'ctx-rh-precuneus', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiortemporal', 'ctx-rh-frontalpole', 'ctx-rh-insula']\n",
      "Number of features selected: 54\n"
     ]
    }
   ],
   "source": [
    "# Genetic + Logistic Regression \n",
    "# Load Original dataset\n",
    "data = pd.read_csv(\"synth_seg.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Check for existing creator classes to avoid redefinition warnings [Use this part only when there is multiple class creation error]\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the accuracy score\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)  # Each individual represents a feature subset\n",
    "\n",
    "# Function to evaluate a feature subset\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid selecting no features\n",
    "\n",
    "    # Select features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Scale the features for better optimization convergence\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    classifier = LogisticRegression(max_iter=5000)  # Increased max_iter to 5000\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy = cross_val_score(classifier, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "    \n",
    "    return accuracy,\n",
    "\n",
    "# Generate an individual (random feature subset)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "# Define crossover, mutation, and selection\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Mutation with 5% probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection via tournament\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run the Genetic Algorithm with the main function\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    pop = toolbox.population(n=50)  # Create a population of 50 individuals\n",
    "    hof = tools.HallOfFame(1)  # Store the best individual\n",
    "\n",
    "    # Run the GA\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]  # Return the best feature subset\n",
    "\n",
    "best_individual = main()\n",
    "\n",
    "# Print the best feature subset found\n",
    "selected_features_lr = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Selected Features from Original dataset using Logistic: {X.columns[selected_features_lr].tolist()}\")\n",
    "print(f\"Number of features selected: {len(selected_features_lr)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c4b35-cc72-4bf1-8c24-5922f8df81a8",
   "metadata": {},
   "source": [
    "Let's use LDA after logistic (Integrating both Logistic and LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4d0c68a-575d-4339-9c89-1683bb7a6fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t25    \n",
      "2  \t19    \n",
      "3  \t27    \n",
      "4  \t31    \n",
      "5  \t34    \n",
      "6  \t32    \n",
      "7  \t37    \n",
      "8  \t31    \n",
      "9  \t31    \n",
      "10 \t32    \n",
      "Selected Features from Original Dataset using Logistic + LDA: ['general white matter', 'general grey matter', 'cerebellum', 'brainstem', 'thalamus', 'hippocampus+amygdala', 'left lateral ventricle', 'left inferior lateral ventricle', 'left cerebellum cortex', 'left thalamus', 'left pallidum', 'left hippocampus', 'left amygdala', 'left ventral DC', 'right inferior lateral ventricle', 'right cerebellum white matter', 'right cerebellum cortex', 'right thalamus', 'right pallidum', 'right hippocampus', 'right amygdala', 'right accumbens area', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-isthmuscingulate', 'ctx-lh-paracentral', 'ctx-lh-parsorbitalis', 'ctx-lh-posteriorcingulate', 'ctx-lh-precuneus', 'ctx-lh-superiorfrontal', 'ctx-lh-frontalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-cuneus', 'ctx-rh-inferiorparietal', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-lingual', 'ctx-rh-parahippocampal', 'ctx-rh-paracentral', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-superiorparietal', 'ctx-rh-transversetemporal', 'ctx-rh-insula']\n",
      "Number of features selected: 50\n"
     ]
    }
   ],
   "source": [
    "# Genetic + Logistic Regression +LDA\n",
    "# Load Original dataset\n",
    "data = pd.read_csv(\"synth_seg.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Check for existing creator classes to avoid redefinition warnings\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the accuracy score\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)  # Each individual represents a feature subset\n",
    "\n",
    "# Function to evaluate a feature subset\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid selecting no features\n",
    "\n",
    "    # Select features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Scale the features for better optimization convergence\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    classifier = LogisticRegression(max_iter=5000)  # Increased max_iter to 5000\n",
    "    \n",
    "    # Cross-validation for Logistic Regression\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy_lr = cross_val_score(classifier, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "\n",
    "    # Integrating LDA with logistic to the selected features\n",
    "    lda = LDA(n_components=1)\n",
    "    accuracy_lda = cross_val_score(lda, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "\n",
    "    # Combine both Logistic Regression and LDA accuracy as fitness\n",
    "    return (accuracy_lr + accuracy_lda) / 2,  # Average the scores from LR and LDA\n",
    "\n",
    "# Generate an individual (random feature subset)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "# Define crossover, mutation, and selection\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Mutation with 5% probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection via tournament\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run the Genetic Algorithm with the main function\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    pop = toolbox.population(n=50)  # Create a population of 50 individuals\n",
    "    hof = tools.HallOfFame(1)  # Store the best individual\n",
    "\n",
    "    # Run the GA\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]  # Return the best feature subset\n",
    "\n",
    "best_individual = main()\n",
    "\n",
    "# Print the best feature subset found\n",
    "selected_features_lr_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Selected Features from Original Dataset using Logistic + LDA: {X.columns[selected_features_lr_lda].tolist()}\")\n",
    "print(f\"Number of features selected: {len(selected_features_lr_lda)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f568625a-811d-44e9-85db-d099511faada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features: {4, 5, 14, 18, 22, 30, 40, 41, 46, 49, 56, 62, 64, 73, 74, 78, 81, 84, 85, 89, 91, 92, 98, 108}\n",
      "Unique to GA + Logistic: {2, 6, 8, 9, 10, 17, 20, 24, 27, 28, 29, 34, 42, 43, 45, 51, 53, 59, 60, 61, 63, 65, 68, 69, 77, 93, 96, 100, 103, 105}\n",
      "Unique to GA + Logistic + LDA: {0, 1, 3, 7, 11, 12, 15, 23, 26, 31, 32, 33, 36, 37, 38, 39, 47, 58, 67, 71, 75, 86, 90, 99, 102, 107}\n"
     ]
    }
   ],
   "source": [
    "# Converting both lists to sets for comparison\n",
    "set_lr_only = set(selected_features_lr)\n",
    "set_lr_lda = set(selected_features_lr_lda)\n",
    "\n",
    "# Features common in both methods\n",
    "common_features = set_lr_only.intersection(set_lr_lda)\n",
    "\n",
    "# Features unique to GA + Logistic\n",
    "unique_lr_only = set_lr_only.difference(set_lr_lda)\n",
    "\n",
    "# Features unique to GA + Logistic + LDA\n",
    "unique_lr_lda = set_lr_lda.difference(set_lr_only)\n",
    "\n",
    "# Print the comparison results\n",
    "print(f\"Common Features: {common_features}\")\n",
    "print(f\"Unique to GA + Logistic: {unique_lr_only}\")\n",
    "print(f\"Unique to GA + Logistic + LDA: {unique_lr_lda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e98985-1eed-4f7e-9cb9-e7f568954550",
   "metadata": {},
   "source": [
    "The above results was for our original dataset now let's try the same for Oversampled and undersampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49cf1b12-3f9b-4a00-b439-a65a4ee58536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t25    \n",
      "2  \t19    \n",
      "3  \t27    \n",
      "4  \t31    \n",
      "5  \t34    \n",
      "6  \t32    \n",
      "7  \t37    \n",
      "8  \t31    \n",
      "9  \t31    \n",
      "10 \t32    \n",
      "Selected Features From Oversampled Dataset using Logistic: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'thalamus', 'hippocampus+amygdala', 'left cerebral cortex', 'left lateral ventricle', 'left cerebellum white matter', 'left thalamus', 'left caudate', 'left putamen', 'left pallidum', 'left hippocampus', 'left ventral DC', 'right cerebral white matter', 'right cerebral cortex', 'right lateral ventricle', 'right inferior lateral ventricle', 'right thalamus', 'right caudate', 'right pallidum', 'right hippocampus', 'right accumbens area', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-cuneus', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-inferiortemporal', 'ctx-lh-lateraloccipital', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-lingual', 'ctx-lh-middletemporal', 'ctx-lh-parsopercularis', 'ctx-lh-pericalcarine', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-lingual', 'ctx-rh-parahippocampal', 'ctx-rh-parsopercularis', 'ctx-rh-parstriangularis', 'ctx-rh-pericalcarine', 'ctx-rh-postcentral', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiortemporal', 'ctx-rh-supramarginal', 'ctx-rh-frontalpole']\n",
      "Number of features selected: 64\n"
     ]
    }
   ],
   "source": [
    "# Genetic + Logistic Regression\n",
    "# Load OverSampled dataset\n",
    "data = pd.read_csv(\"resampled_data.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Check for existing creator classes to avoid redefinition warnings [Use this part only when there is multiple class creation error]\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the accuracy score\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)  # Each individual represents a feature subset\n",
    "\n",
    "# Function to evaluate a feature subset\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid selecting no features\n",
    "\n",
    "    # Select features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Scale the features for better optimization convergence\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    classifier = LogisticRegression(max_iter=5000)  # Increased max_iter to 5000\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy = cross_val_score(classifier, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "    \n",
    "    return accuracy,\n",
    "\n",
    "# Generate an individual (random feature subset)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "# Define crossover, mutation, and selection\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Mutation with 5% probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection via tournament\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run the Genetic Algorithm with the main function\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    pop = toolbox.population(n=50)  # Create a population of 50 individuals\n",
    "    hof = tools.HallOfFame(1)  # Store the best individual\n",
    "\n",
    "    # Run the GA\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]  # Return the best feature subset\n",
    "\n",
    "best_individual = main()\n",
    "\n",
    "# Print the best feature subset found\n",
    "selected_features_lr_os = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Selected Features From Oversampled Dataset using Logistic: {X.columns[selected_features_lr_os].tolist()}\")\n",
    "print(f\"Number of features selected: {len(selected_features_lr_os)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "054f07c1-c788-426d-bd82-9b85138ad94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t25    \n",
      "2  \t19    \n",
      "3  \t27    \n",
      "4  \t31    \n",
      "5  \t34    \n",
      "6  \t32    \n",
      "7  \t37    \n",
      "8  \t31    \n",
      "9  \t31    \n",
      "10 \t32    \n",
      "Selected Features From Oversampled Dataset using Logistic + LDA: ['general white matter', 'brainstem', 'thalamus', 'putamen+pallidum', 'total intracranial', 'left cerebral white matter', 'left lateral ventricle', 'left inferior lateral ventricle', 'left cerebellum white matter', 'left cerebellum cortex', 'left thalamus', 'left putamen', 'csf', 'left ventral DC', 'right cerebral white matter', 'right cerebral cortex', 'right lateral ventricle', 'right inferior lateral ventricle', 'right thalamus', 'right caudate', 'right pallidum', 'right hippocampus', 'right accumbens area', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-cuneus', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-lateraloccipital', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-lingual', 'ctx-lh-middletemporal', 'ctx-lh-parstriangularis', 'ctx-lh-pericalcarine', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalanteriorcingulate', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-lingual', 'ctx-rh-parahippocampal', 'ctx-rh-parsopercularis', 'ctx-rh-parstriangularis', 'ctx-rh-pericalcarine', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-precentral', 'ctx-rh-precuneus', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiortemporal', 'ctx-rh-temporalpole']\n",
      "Number of features selected: 65\n"
     ]
    }
   ],
   "source": [
    "# Genetic + Logistic Regression +LDA\n",
    "# Load OverSampled dataset\n",
    "data = pd.read_csv(\"resampled_data.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Check for existing creator classes to avoid redefinition warnings\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the accuracy score\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)  # Each individual represents a feature subset\n",
    "\n",
    "# Function to evaluate a feature subset\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid selecting no features\n",
    "\n",
    "    # Select features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Scale the features for better optimization convergence\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    classifier = LogisticRegression(max_iter=5000)  # Increased max_iter to 5000\n",
    "    \n",
    "    # Cross-validation for Logistic Regression\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy_lr = cross_val_score(classifier, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "\n",
    "    # Integrating LDA with logistic to the selected features\n",
    "    lda = LDA(n_components=1)\n",
    "    accuracy_lda = cross_val_score(lda, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "\n",
    "    # Combine both Logistic Regression and LDA accuracy as fitness\n",
    "    return (accuracy_lr + accuracy_lda) / 2,  # Average the scores from LR and LDA\n",
    "\n",
    "# Generate an individual (random feature subset)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "# Define crossover, mutation, and selection\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Mutation with 5% probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection via tournament\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run the Genetic Algorithm with the main function\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    pop = toolbox.population(n=50)  # Create a population of 50 individuals\n",
    "    hof = tools.HallOfFame(1)  # Store the best individual\n",
    "\n",
    "    # Run the GA\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]  # Return the best feature subset\n",
    "\n",
    "best_individual = main()\n",
    "\n",
    "# Print the best feature subset found\n",
    "selected_features_lr_lda_os = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Selected Features From Oversampled Dataset using Logistic + LDA: {X.columns[selected_features_lr_lda_os].tolist()}\")\n",
    "print(f\"Number of features selected: {len(selected_features_lr_lda_os)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a90b10af-6b45-452a-89b3-f1d7f7d35cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features: {0, 4, 5, 11, 13, 15, 17, 26, 27, 28, 29, 30, 33, 34, 36, 37, 39, 41, 42, 44, 46, 47, 50, 51, 52, 54, 60, 62, 63, 65, 66, 67, 69, 70, 71, 72, 74, 75, 77, 81, 82, 85, 86, 89, 91, 93, 94, 95, 100, 103}\n",
      "Unique to GA + Logistic: {1, 2, 3, 7, 104, 73, 10, 105, 79, 16, 48, 18, 22, 57}\n",
      "Unique to GA + Logistic + LDA: {64, 96, 97, 98, 6, 8, 9, 40, 106, 12, 45, 14, 76, 24, 59}\n"
     ]
    }
   ],
   "source": [
    "# Converting both lists to sets for comparison\n",
    "set_lr_only_os = set(selected_features_lr_os)\n",
    "set_lr_lda_os = set(selected_features_lr_lda_os)\n",
    "\n",
    "# Features common in both methods\n",
    "common_features_os = set_lr_only_os.intersection(set_lr_lda_os)\n",
    "\n",
    "# Features unique to GA + Logistic\n",
    "unique_lr_only_os = set_lr_only_os.difference(set_lr_lda_os)\n",
    "\n",
    "# Features unique to GA + Logistic + LDA\n",
    "unique_lr_lda_os = set_lr_lda_os.difference(set_lr_only_os)\n",
    "\n",
    "# Print the comparison results\n",
    "print(f\"Common Features: {common_features_os}\")\n",
    "print(f\"Unique to GA + Logistic: {unique_lr_only_os}\")\n",
    "print(f\"Unique to GA + Logistic + LDA: {unique_lr_lda_os}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452cd6f-9f8c-4fef-a0c1-3d646ae9087a",
   "metadata": {},
   "source": [
    "Now let's do the same for UNDERSAMPLED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36dd7816-ac38-4d89-96a5-1d84a33c556d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t25    \n",
      "2  \t19    \n",
      "3  \t27    \n",
      "4  \t31    \n",
      "5  \t34    \n",
      "6  \t32    \n",
      "7  \t37    \n",
      "8  \t31    \n",
      "9  \t31    \n",
      "10 \t32    \n",
      "Selected Features From Undersampled Dataset using Logistic : ['general white matter', 'general grey matter', 'brainstem', 'left cerebral white matter', 'left inferior lateral ventricle', 'left cerebellum white matter', 'left cerebellum cortex', 'left thalamus', 'left caudate', 'left putamen', 'left pallidum', '4th ventricle', 'brain-stem', 'left ventral DC', 'right lateral ventricle', 'right cerebellum white matter', 'right cerebellum cortex', 'right caudate', 'right pallidum', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-caudalmiddlefrontal', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-lateraloccipital', 'ctx-lh-lingual', 'ctx-lh-middletemporal', 'ctx-lh-parsopercularis', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-isthmuscingulate', 'ctx-rh-parsopercularis', 'ctx-rh-pericalcarine', 'ctx-rh-postcentral', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-superiorfrontal', 'ctx-rh-superiorparietal', 'ctx-rh-superiortemporal', 'ctx-rh-supramarginal', 'ctx-rh-insula']\n",
      "Number of features selected: 56\n"
     ]
    }
   ],
   "source": [
    "# Genetic + Logistic Regression\n",
    "# Load UnderSampled dataset\n",
    "data = pd.read_csv(\"undersampled_data.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Check for existing creator classes to avoid redefinition warnings [Use this part only when there is multiple class creation error]\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the accuracy score\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)  # Each individual represents a feature subset\n",
    "\n",
    "# Function to evaluate a feature subset\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid selecting no features\n",
    "\n",
    "    # Select features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Scale the features for better optimization convergence\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    classifier = LogisticRegression(max_iter=5000)  # Increased max_iter to 5000\n",
    "    \n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy = cross_val_score(classifier, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "    \n",
    "    return accuracy,\n",
    "\n",
    "# Generate an individual (random feature subset)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "# Define crossover, mutation, and selection\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Mutation with 5% probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection via tournament\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run the Genetic Algorithm with the main function\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    pop = toolbox.population(n=50)  # Create a population of 50 individuals\n",
    "    hof = tools.HallOfFame(1)  # Store the best individual\n",
    "\n",
    "    # Run the GA\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]  # Return the best feature subset\n",
    "\n",
    "best_individual = main()\n",
    "\n",
    "# Print the best feature subset found\n",
    "selected_features_lr_us = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Selected Features From Undersampled Dataset using Logistic : {X.columns[selected_features_lr_us].tolist()}\")\n",
    "print(f\"Number of features selected: {len(selected_features_lr_us)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af1dc4e6-ea26-47b2-a028-ee2c7186ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t50    \n",
      "1  \t25    \n",
      "2  \t19    \n",
      "3  \t27    \n",
      "4  \t31    \n",
      "5  \t34    \n",
      "6  \t32    \n",
      "7  \t37    \n",
      "8  \t31    \n",
      "9  \t31    \n",
      "10 \t32    \n",
      "Selected Features From Undersampled Dataset using Logistic + LDA: ['general grey matter', 'brainstem', 'left cerebral white matter', 'left inferior lateral ventricle', 'left cerebellum white matter', 'left cerebellum cortex', 'left thalamus', 'left caudate', 'left putamen', 'left pallidum', '4th ventricle', 'left ventral DC', 'right cerebral cortex', 'right lateral ventricle', 'right cerebellum white matter', 'right cerebellum cortex', 'right thalamus', 'right pallidum', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-caudalmiddlefrontal', 'ctx-lh-cuneus', 'ctx-lh-lateraloccipital', 'ctx-lh-lingual', 'ctx-lh-paracentral', 'ctx-lh-parsopercularis', 'ctx-lh-parstriangularis', 'ctx-lh-pericalcarine', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-transversetemporal', 'ctx-rh-cuneus', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-lingual', 'ctx-rh-parahippocampal', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-precentral', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-superiortemporal', 'ctx-rh-supramarginal', 'ctx-rh-transversetemporal', 'ctx-rh-insula']\n",
      "Number of features selected: 50\n"
     ]
    }
   ],
   "source": [
    "# Genetic + Logistic Regression +LDA\n",
    "# Load UnderSampled dataset\n",
    "data = pd.read_csv(\"undersampled_data.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Check for existing creator classes to avoid redefinition warnings\n",
    "if not hasattr(creator, \"FitnessMax\"):\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize the accuracy score\n",
    "if not hasattr(creator, \"Individual\"):\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)  # Each individual represents a feature subset\n",
    "\n",
    "# Function to evaluate a feature subset\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    \n",
    "    if len(selected_features) == 0:\n",
    "        return 0,  # Avoid selecting no features\n",
    "\n",
    "    # Select features\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    \n",
    "    # Scale the features for better optimization convergence\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "    \n",
    "    # Logistic Regression\n",
    "    classifier = LogisticRegression(max_iter=5000)  # Increased max_iter to 5000\n",
    "    \n",
    "    # Cross-validation for Logistic Regression\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    accuracy_lr = cross_val_score(classifier, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "\n",
    "    # Integrating LDA with logistic to the selected features\n",
    "    lda = LDA(n_components=1)\n",
    "    accuracy_lda = cross_val_score(lda, X_scaled, y, cv=skf, scoring='accuracy').mean()\n",
    "\n",
    "    # Combine both Logistic Regression and LDA accuracy as fitness\n",
    "    return (accuracy_lr + accuracy_lda) / 2,  # Average the scores from LR and LDA\n",
    "\n",
    "# Generate an individual (random feature subset)\n",
    "def create_individual():\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "# Define crossover, mutation, and selection\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Register the genetic operators\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Mutation with 5% probability\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Selection via tournament\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# Run the Genetic Algorithm with the main function\n",
    "def main():\n",
    "    random.seed(42)\n",
    "    pop = toolbox.population(n=50)  # Create a population of 50 individuals\n",
    "    hof = tools.HallOfFame(1)  # Store the best individual\n",
    "\n",
    "    # Run the GA\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True)\n",
    "\n",
    "    return hof[0]  # Return the best feature subset\n",
    "\n",
    "best_individual = main()\n",
    "\n",
    "# Print the best feature subset found\n",
    "selected_features_lr_lda_us = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Selected Features From Undersampled Dataset using Logistic + LDA: {X.columns[selected_features_lr_lda_us].tolist()}\")\n",
    "print(f\"Number of features selected: {len(selected_features_lr_lda_us)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "833f2b6d-cebe-4693-90c9-0b96c969ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Features: {1, 4, 9, 12, 13, 14, 15, 16, 17, 18, 20, 26, 29, 31, 32, 36, 40, 41, 43, 50, 52, 57, 63, 64, 65, 66, 73, 81, 82, 95, 98, 99, 103, 104, 108}\n",
      "Unique to GA + Logistic: {0, 21, 34, 42, 45, 46, 47, 54, 67, 69, 70, 71, 72, 74, 75, 77, 83, 91, 94, 101, 102}\n",
      "Unique to GA + Logistic + LDA: {96, 33, 97, 107, 44, 60, 78, 84, 85, 86, 56, 89, 59, 28, 62}\n"
     ]
    }
   ],
   "source": [
    "# Converting both lists to sets for comparison\n",
    "set_lr_only_us = set(selected_features_lr_us)\n",
    "set_lr_lda_us = set(selected_features_lr_lda_us)\n",
    "\n",
    "# Features common in both methods\n",
    "common_features_us = set_lr_only_us.intersection(set_lr_lda_us)\n",
    "\n",
    "# Features unique to GA + Logistic\n",
    "unique_lr_only_us = set_lr_only_us.difference(set_lr_lda_us)\n",
    "\n",
    "# Features unique to GA + Logistic + LDA\n",
    "unique_lr_lda_us = set_lr_lda_us.difference(set_lr_only_us)\n",
    "\n",
    "# Print the comparison results\n",
    "print(f\"Common Features: {common_features_us}\")\n",
    "print(f\"Unique to GA + Logistic: {unique_lr_only_us}\")\n",
    "print(f\"Unique to GA + Logistic + LDA: {unique_lr_lda_us}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
