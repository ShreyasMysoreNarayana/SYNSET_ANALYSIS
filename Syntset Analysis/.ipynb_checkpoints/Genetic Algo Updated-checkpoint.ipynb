{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9a57fc-7949-4e86-b228-24548a4e6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6427f2-0ef5-4462-b219-d6fdac3e3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at generation 2\n",
      "Final Best Solution (found at generation 1):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'thalamus', 'hippocampus+amygdala', 'left lateral ventricle', 'left inferior lateral ventricle', 'left cerebellum white matter', 'left thalamus', 'left putamen', 'left hippocampus', 'left amygdala', 'right inferior lateral ventricle', 'right accumbens area', 'ctx-lh-bankssts', 'ctx-lh-caudalmiddlefrontal', 'ctx-lh-cuneus', 'ctx-lh-entorhinal', 'ctx-lh-inferiorparietal', 'ctx-lh-inferiortemporal', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-lingual', 'ctx-lh-parsopercularis', 'ctx-lh-parstriangularis', 'ctx-lh-postcentral', 'ctx-lh-posteriorcingulate', 'ctx-lh-precuneus', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-temporalpole', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalanteriorcingulate', 'ctx-rh-cuneus', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-lingual', 'ctx-rh-parsorbitalis', 'ctx-rh-parstriangularis', 'ctx-rh-postcentral', 'ctx-rh-precuneus']\n",
      "  Number of features selected: 46\n",
      "\n",
      "Fitness values across generations:\n",
      "Generation 0: 0.7018543645409318\n",
      "Generation 1: 0.7137042062415198\n",
      "Generation 2: 0.7137042062415198\n"
     ]
    }
   ],
   "source": [
    "#Run the code for Origial Dataset\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"synth_seg.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features):\n",
    "    # Creates a random individual (feature subset).\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "def apply_lda(numeric_df, y, score):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "\n",
    "    # Remove train_test_split, we will use cross-validation directly on the entire dataset\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df =  pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "        different_locs = np.where(y_test_fold != y_pred_fold)[0]\n",
    "\n",
    "        # Map these back to the original indices in lda_df\n",
    "        original_different_locs = test_index[different_locs]\n",
    "\n",
    "    return sum(accuracy_scores) / len(accuracy_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy_lda = apply_lda(pd.DataFrame(X_scaled), y, data['neuropsych_score'])  # Pass scaled data as DataFrame\n",
    "\n",
    "    return accuracy_lda  # Use only LDA accuracy here\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def mutation(individual, indpb=0.05):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    return (individual,)\n",
    "\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i])\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(\"\\nFitness values across generations:\")\n",
    "for gen, fitness in enumerate(best_fitnesses):\n",
    "    print(f\"Generation {gen}: {fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bab2f7d-2181-4256-b494-e426c8218f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at generation 2\n",
      "Final Best Solution (found at generation 1):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'thalamus', 'putamen+pallidum', 'left cerebral cortex', 'left lateral ventricle', 'left inferior lateral ventricle', 'left cerebellum white matter', 'left thalamus', 'left pallidum', '3rd ventricle', 'brain-stem', 'left hippocampus', 'csf', 'left accumbens area', 'left ventral DC', 'right lateral ventricle', 'right inferior lateral ventricle', 'right cerebellum cortex', 'right caudate', 'right putamen', 'right pallidum', 'right hippocampus', 'right amygdala', 'right accumbens area', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-cuneus', 'ctx-lh-entorhinal', 'ctx-lh-inferiorparietal', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateraloccipital', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-paracentral', 'ctx-lh-pericalcarine', 'ctx-lh-postcentral', 'ctx-lh-posteriorcingulate', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-fusiform', 'ctx-rh-parsopercularis', 'ctx-rh-pericalcarine', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiortemporal', 'ctx-rh-supramarginal', 'ctx-rh-frontalpole', 'ctx-rh-temporalpole', 'ctx-rh-transversetemporal']\n",
      "  Number of features selected: 64\n",
      "\n",
      "Fitness values across generations:\n",
      "Generation 0: 0.7159793814432989\n",
      "Generation 1: 0.7304553264604812\n",
      "Generation 2: 0.7304553264604812\n"
     ]
    }
   ],
   "source": [
    "#Run code for the Over Sampled Data\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"resampled_data.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features):\n",
    "    # Creates a random individual (feature subset).\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "def apply_lda(numeric_df, y, score):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "\n",
    "    # Remove train_test_split, we will use cross-validation directly on the entire dataset\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df =  pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "        different_locs = np.where(y_test_fold != y_pred_fold)[0]\n",
    "\n",
    "        # Map these back to the original indices in lda_df\n",
    "        original_different_locs = test_index[different_locs]\n",
    "\n",
    "    return sum(accuracy_scores) / len(accuracy_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy_lda = apply_lda(pd.DataFrame(X_scaled), y, data['neuropsych_score'])  # Pass scaled data as DataFrame\n",
    "\n",
    "    return accuracy_lda  # Use only LDA accuracy here\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def mutation(individual, indpb=0.05):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    return (individual,)\n",
    "\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i])\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(\"\\nFitness values across generations:\")\n",
    "for gen, fitness in enumerate(best_fitnesses):\n",
    "    print(f\"Generation {gen}: {fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa49765-ae21-41e6-8f81-0e7a111e5192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at generation 3\n",
      "Final Best Solution (found at generation 2):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'left cerebral white matter', 'left lateral ventricle', 'left inferior lateral ventricle', 'left cerebellum white matter', 'left pallidum', '3rd ventricle', 'left hippocampus', 'left amygdala', 'left accumbens area', 'right cerebral white matter', 'right inferior lateral ventricle', 'right cerebellum cortex', 'right thalamus', 'right caudate', 'right pallidum', 'right amygdala', 'right ventral DC', 'ctx-lh-bankssts', 'ctx-lh-fusiform', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-lingual', 'ctx-lh-medialorbitofrontal', 'ctx-lh-middletemporal', 'ctx-lh-parahippocampal', 'ctx-lh-parsopercularis', 'ctx-lh-parstriangularis', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-superiorparietal', 'ctx-lh-superiortemporal', 'ctx-lh-frontalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-cuneus', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-isthmuscingulate', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-parahippocampal', 'ctx-rh-paracentral', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-parstriangularis', 'ctx-rh-posteriorcingulate', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-rostralmiddlefrontal']\n",
      "  Number of features selected: 57\n",
      "\n",
      "Fitness values across generations:\n",
      "Generation 0: 0.5882882882882883\n",
      "Generation 1: 0.621021021021021\n",
      "Generation 2: 0.6373873873873873\n",
      "Generation 3: 0.6373873873873873\n"
     ]
    }
   ],
   "source": [
    "#Run the Code for Under Sampled Dataset\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"undersampled_data.csv\")\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features):\n",
    "    # Creates a random individual (feature subset).\n",
    "    return [random.randint(0, 1) for _ in range(n_features)]\n",
    "\n",
    "def apply_lda(numeric_df, y, score):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "\n",
    "    # Remove train_test_split, we will use cross-validation directly on the entire dataset\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df =  pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "        different_locs = np.where(y_test_fold != y_pred_fold)[0]\n",
    "\n",
    "        # Map these back to the original indices in lda_df\n",
    "        original_different_locs = test_index[different_locs]\n",
    "\n",
    "    return sum(accuracy_scores) / len(accuracy_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy_lda = apply_lda(pd.DataFrame(X_scaled), y, data['neuropsych_score'])  # Pass scaled data as DataFrame\n",
    "\n",
    "    return accuracy_lda  # Use only LDA accuracy here\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "\n",
    "def mutation(individual, indpb=0.05):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    return (individual,)\n",
    "\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i])\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(\"\\nFitness values across generations:\")\n",
    "for gen, fitness in enumerate(best_fitnesses):\n",
    "    print(f\"Generation {gen}: {fitness}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7209a-0acc-4b0b-a4ed-d86ce190965b",
   "metadata": {},
   "source": [
    "## Key Changes in the code  \n",
    "\n",
    "### LDA Implementation without Data Leakage Fix:\n",
    "\n",
    "The initial code applied LDA to the entire dataset before cross-validation, potentially causing data leakage.\n",
    "\n",
    "The apply_lda function was introduced to perform LDA and Logistic Regression within each cross-validation fold, preventing data leakage and providing more accurate fitness evaluations.\n",
    "\n",
    "### Mutation at a Random Point:\n",
    "\n",
    "Initially, the mutation operation flipped bits of all features in a selected individual with a certain probability.\n",
    "\n",
    "The mutation operation was modified to select a single random point (feature) in the individual and flip the bit at that point. This introduces more focused changes during mutation.\n",
    "\n",
    "### Selection:\n",
    "\n",
    "The initial code used tournament selection to randomly choose individuals for the next generation.\n",
    "\n",
    "Elitism selection was introduced, where the top-performing individuals from each generation are directly passed on to the next generation, ensuring the preservation of high-quality solutions.\n",
    "\n",
    "### Crossover from Best Individuals:\n",
    "\n",
    "Initially, crossover was performed on randomly selected individuals from the population.\n",
    "\n",
    "The crossover operation was modified to select parents only from the top-performing individuals, promoting the combination of good traits and potentially creating even better offspring.\n",
    "\n",
    "The original top-performing individuals are preserved in the next generation, and crossover is used to create new offspring to fill the rest of the population.\n",
    "\n",
    "### Fitness Tracking and Convergence Check:\n",
    "\n",
    "The best_fitnesses list was added to store the fitness value of the best individual in each generation.\n",
    "\n",
    "A convergence check was introduced to stop the genetic algorithm when the fitness value converges (i.e., the change in fitness between generations becomes very small).\n",
    "\n",
    "The code now prints the fitness values across generations to show how the fitness improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7974d-c86f-4f50-95a5-44828c91c58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
