{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9a57fc-7949-4e86-b228-24548a4e6238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f2cbe-21a4-44c2-af85-b42bd961c309",
   "metadata": {},
   "source": [
    "## First let's run the code on the Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5bf8ff7-c630-4d13-8397-bd4dd872642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose feature handling option for doctor's preferred features:\n",
      "1: Keep doctor's preferred features as non-mutable (always ON)\n",
      "2: Initialize doctor's preferred features as ON but allow mutation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: Fitness = 1.9378383522381866, Accuracy = 0.7078697421981005\n",
      "Generation 1: Fitness = 1.977845442588286, Accuracy = 0.7135232926277703\n",
      "Generation 2: Fitness = 1.977845442588286, Accuracy = 0.7135232926277703\n",
      "Converged at generation 2\n",
      "Final Best Solution (found at generation 1):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'left cerebellum cortex', 'left thalamus', 'left pallidum', '3rd ventricle', '4th ventricle', 'left hippocampus', 'left amygdala', 'left ventral DC', 'right cerebral white matter', 'right lateral ventricle', 'right hippocampus', 'right amygdala', 'right accumbens area', 'right ventral DC', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-caudalmiddlefrontal', 'ctx-lh-cuneus', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-inferiortemporal', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-medialorbitofrontal', 'ctx-lh-parstriangularis', 'ctx-lh-pericalcarine', 'ctx-lh-postcentral', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-frontalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-cuneus', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-isthmuscingulate', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-parahippocampal', 'ctx-rh-paracentral', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiorparietal', 'ctx-rh-frontalpole', 'ctx-rh-temporalpole']\n",
      "  Number of features selected: 62\n",
      "Best fitness values across generations: [1.9378383522381866, 1.977845442588286, 1.977845442588286]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"synth_seg.csv\")  # Make sure the dataset is available in this path\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Doctor's preferred features (to be handled differently)\n",
    "preferred_features = [\n",
    "    'general grey matter', 'general csf', 'cerebellum', 'hippocampus+amygdala',\n",
    "    'left hippocampus', 'right hippocampus', 'ctx-lh-entorhinal', 'ctx-rh-entorhinal'\n",
    "]\n",
    "\n",
    "# Mapping feature names to their indices in the dataset\n",
    "preferred_feature_indices = [X.columns.get_loc(feature) for feature in preferred_features]\n",
    "\n",
    "# User input to decide the option (Option 1 or Option 2)\n",
    "print(\"Choose feature handling option for doctor's preferred features:\")\n",
    "print(\"1: Keep doctor's preferred features as non-mutable (always ON)\")\n",
    "print(\"2: Initialize doctor's preferred features as ON but allow mutation\")\n",
    "user_choice = int(input(\"Enter your choice (1 or 2): \"))\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features, preferred_feature_indices, user_choice):\n",
    "    # Creates a random individual (feature subset).\n",
    "    individual = [random.randint(0, 1) for _ in range(n_features)]\n",
    "    \n",
    "    if user_choice == 1:  # Option 1: Set preferred features to 1 and non-mutable\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    elif user_choice == 2:  # Option 2: Set preferred features to 1 initially, but allow mutation later\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def apply_lda(numeric_df, y):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df = pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "\n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    num_selected_features = len(selected_features)\n",
    "    \n",
    "    if num_selected_features == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy, precision, recall = apply_lda(pd.DataFrame(X_scaled), y)\n",
    "\n",
    "    # Calculate fitness with penalty for more features\n",
    "    fitness = accuracy + precision + recall + (1 / num_selected_features) \n",
    "    return fitness\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(individual, indpb=0.05, preferred_feature_indices=None, user_choice=None):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        \n",
    "        # Option 1: Keep doctor's preferred features as non-mutable\n",
    "        if user_choice == 1 and point in preferred_feature_indices:\n",
    "            return individual  # Skip mutation for preferred features\n",
    "\n",
    "        # Option 2: Allow mutation on all features, including the preferred ones\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    \n",
    "    return (individual,)\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features, preferred_feature_indices, user_choice) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i], preferred_feature_indices=preferred_feature_indices, user_choice=user_choice)\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Print generation information for the top individual\n",
    "        best_individual_current_gen = max(population, key=lambda ind: evaluate(ind, X, y))\n",
    "        \n",
    "        # Calculate X_scaled for the best individual\n",
    "        selected_features = [i for i, bit in enumerate(best_individual_current_gen) if bit == 1]\n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "        accuracy_top = apply_lda(pd.DataFrame(X_scaled), y)[0]  # Get accuracy for the top individual\n",
    "        print(f\"Generation {gen}: Fitness = {best_fitnesses[-1]}, Accuracy = {accuracy_top}\") \n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(f\"Best fitness values across generations: {best_fitnesses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc12c6f-49f5-4131-a6e0-96d387dad110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose feature handling option for doctor's preferred features:\n",
      "1: Keep doctor's preferred features as non-mutable (always ON)\n",
      "2: Initialize doctor's preferred features as ON but allow mutation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: Fitness = 1.9378383522381866, Accuracy = 0.7078697421981005\n",
      "Generation 1: Fitness = 1.977845442588286, Accuracy = 0.7135232926277703\n",
      "Generation 2: Fitness = 1.977845442588286, Accuracy = 0.7135232926277703\n",
      "Converged at generation 2\n",
      "Final Best Solution (found at generation 1):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'left cerebellum cortex', 'left thalamus', 'left pallidum', '3rd ventricle', '4th ventricle', 'left hippocampus', 'left amygdala', 'left ventral DC', 'right cerebral white matter', 'right lateral ventricle', 'right hippocampus', 'right amygdala', 'right accumbens area', 'right ventral DC', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-caudalmiddlefrontal', 'ctx-lh-cuneus', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-inferiortemporal', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-medialorbitofrontal', 'ctx-lh-parstriangularis', 'ctx-lh-pericalcarine', 'ctx-lh-postcentral', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-rostralanteriorcingulate', 'ctx-lh-rostralmiddlefrontal', 'ctx-lh-superiorfrontal', 'ctx-lh-superiortemporal', 'ctx-lh-frontalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-cuneus', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-isthmuscingulate', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-parahippocampal', 'ctx-rh-paracentral', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiorparietal', 'ctx-rh-frontalpole', 'ctx-rh-temporalpole']\n",
      "  Number of features selected: 62\n",
      "Best fitness values across generations: [1.9378383522381866, 1.977845442588286, 1.977845442588286]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"synth_seg.csv\")  # Make sure the dataset is available in this path\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Doctor's preferred features (to be handled differently)\n",
    "preferred_features = [\n",
    "    'general grey matter', 'general csf', 'cerebellum', 'hippocampus+amygdala',\n",
    "    'left hippocampus', 'right hippocampus', 'ctx-lh-entorhinal', 'ctx-rh-entorhinal'\n",
    "]\n",
    "\n",
    "# Mapping feature names to their indices in the dataset\n",
    "preferred_feature_indices = [X.columns.get_loc(feature) for feature in preferred_features]\n",
    "\n",
    "# User input to decide the option (Option 1 or Option 2)\n",
    "print(\"Choose feature handling option for doctor's preferred features:\")\n",
    "print(\"1: Keep doctor's preferred features as non-mutable (always ON)\")\n",
    "print(\"2: Initialize doctor's preferred features as ON but allow mutation\")\n",
    "user_choice = int(input(\"Enter your choice (1 or 2): \"))\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features, preferred_feature_indices, user_choice):\n",
    "    # Creates a random individual (feature subset).\n",
    "    individual = [random.randint(0, 1) for _ in range(n_features)]\n",
    "    \n",
    "    if user_choice == 1:  # Option 1: Set preferred features to 1 and non-mutable\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    elif user_choice == 2:  # Option 2: Set preferred features to 1 initially, but allow mutation later\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def apply_lda(numeric_df, y):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df = pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "\n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    num_selected_features = len(selected_features)\n",
    "    \n",
    "    if num_selected_features == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy, precision, recall = apply_lda(pd.DataFrame(X_scaled), y)\n",
    "\n",
    "    # Calculate fitness with penalty for more features\n",
    "    fitness = accuracy + precision + recall + (1 / num_selected_features) \n",
    "    return fitness\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(individual, indpb=0.05, preferred_feature_indices=None, user_choice=None):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        \n",
    "        # Option 1: Keep doctor's preferred features as non-mutable\n",
    "        if user_choice == 1 and point in preferred_feature_indices:\n",
    "            return individual  # Skip mutation for preferred features\n",
    "\n",
    "        # Option 2: Allow mutation on all features, including the preferred ones\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    \n",
    "    return (individual,)\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features, preferred_feature_indices, user_choice) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i], preferred_feature_indices=preferred_feature_indices, user_choice=user_choice)\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Print generation information for the top individual\n",
    "        best_individual_current_gen = max(population, key=lambda ind: evaluate(ind, X, y))\n",
    "        \n",
    "        # Calculate X_scaled for the best individual\n",
    "        selected_features = [i for i, bit in enumerate(best_individual_current_gen) if bit == 1]\n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "        accuracy_top = apply_lda(pd.DataFrame(X_scaled), y)[0]  # Get accuracy for the top individual\n",
    "        print(f\"Generation {gen}: Fitness = {best_fitnesses[-1]}, Accuracy = {accuracy_top}\") \n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(f\"Best fitness values across generations: {best_fitnesses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066abbd1-ea39-4d26-812a-26a91633d95f",
   "metadata": {},
   "source": [
    "## Now let's run the code on OverSampled Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825e423b-fda6-4661-bc79-2aa0b1aaa643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose feature handling option for doctor's preferred features:\n",
      "1: Keep doctor's preferred features as non-mutable (always ON)\n",
      "2: Initialize doctor's preferred features as ON but allow mutation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: Fitness = 2.185289937662087, Accuracy = 0.717955326460481\n",
      "Generation 1: Fitness = 2.239749012985335, Accuracy = 0.7365979381443298\n",
      "Generation 2: Fitness = 2.2786872220064778, Accuracy = 0.7490120274914089\n",
      "Generation 3: Fitness = 2.280094051447716, Accuracy = 0.7510953608247424\n",
      "Generation 4: Fitness = 2.297304998266381, Accuracy = 0.7552835051546392\n",
      "Generation 5: Fitness = 2.297304998266381, Accuracy = 0.7552835051546392\n",
      "Converged at generation 5\n",
      "Final Best Solution (found at generation 4):\n",
      "  Selected Features: ['general grey matter', 'general csf', 'cerebellum', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'left cerebral white matter', 'left cerebral cortex', 'left caudate', 'left putamen', 'left pallidum', '4th ventricle', 'left hippocampus', 'left ventral DC', 'right cerebral white matter', 'right lateral ventricle', 'right inferior lateral ventricle', 'right cerebellum white matter', 'right thalamus', 'right putamen', 'right pallidum', 'right hippocampus', 'right amygdala', 'right accumbens area', 'right ventral DC', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiortemporal', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-lingual', 'ctx-lh-paracentral', 'ctx-lh-parsorbitalis', 'ctx-lh-parstriangularis', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-superiorparietal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-entorhinal', 'ctx-rh-fusiform', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-pericalcarine', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiortemporal', 'ctx-rh-supramarginal', 'ctx-rh-frontalpole']\n",
      "  Number of features selected: 57\n",
      "Best fitness values across generations: [2.185289937662087, 2.239749012985335, 2.2786872220064778, 2.280094051447716, 2.297304998266381, 2.297304998266381]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"resampled_data.csv\")  # Make sure the dataset is available in this path\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Doctor's preferred features (to be handled differently)\n",
    "preferred_features = [\n",
    "    'general grey matter', 'general csf', 'cerebellum', 'hippocampus+amygdala',\n",
    "    'left hippocampus', 'right hippocampus', 'ctx-lh-entorhinal', 'ctx-rh-entorhinal'\n",
    "]\n",
    "\n",
    "# Mapping feature names to their indices in the dataset\n",
    "preferred_feature_indices = [X.columns.get_loc(feature) for feature in preferred_features]\n",
    "\n",
    "# User input to decide the option (Option 1 or Option 2)\n",
    "print(\"Choose feature handling option for doctor's preferred features:\")\n",
    "print(\"1: Keep doctor's preferred features as non-mutable (always ON)\")\n",
    "print(\"2: Initialize doctor's preferred features as ON but allow mutation\")\n",
    "user_choice = int(input(\"Enter your choice (1 or 2): \"))\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features, preferred_feature_indices, user_choice):\n",
    "    # Creates a random individual (feature subset).\n",
    "    individual = [random.randint(0, 1) for _ in range(n_features)]\n",
    "    \n",
    "    if user_choice == 1:  # Option 1: Set preferred features to 1 and non-mutable\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    elif user_choice == 2:  # Option 2: Set preferred features to 1 initially, but allow mutation later\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def apply_lda(numeric_df, y):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df = pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "\n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    num_selected_features = len(selected_features)\n",
    "    \n",
    "    if num_selected_features == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy, precision, recall = apply_lda(pd.DataFrame(X_scaled), y)\n",
    "\n",
    "    # Calculate fitness with penalty for more features\n",
    "    fitness = accuracy + precision + recall + (1 / num_selected_features) \n",
    "    return fitness\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(individual, indpb=0.05, preferred_feature_indices=None, user_choice=None):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        \n",
    "        # Option 1: Keep doctor's preferred features as non-mutable\n",
    "        if user_choice == 1 and point in preferred_feature_indices:\n",
    "            return individual  # Skip mutation for preferred features\n",
    "\n",
    "        # Option 2: Allow mutation on all features, including the preferred ones\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    \n",
    "    return (individual,)\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features, preferred_feature_indices, user_choice) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i], preferred_feature_indices=preferred_feature_indices, user_choice=user_choice)\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Print generation information for the top individual\n",
    "        best_individual_current_gen = max(population, key=lambda ind: evaluate(ind, X, y))\n",
    "        \n",
    "        # Calculate X_scaled for the best individual\n",
    "        selected_features = [i for i, bit in enumerate(best_individual_current_gen) if bit == 1]\n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "        accuracy_top = apply_lda(pd.DataFrame(X_scaled), y)[0]  # Get accuracy for the top individual\n",
    "        print(f\"Generation {gen}: Fitness = {best_fitnesses[-1]}, Accuracy = {accuracy_top}\") \n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(f\"Best fitness values across generations: {best_fitnesses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45812d4-9819-420c-94b9-c0b4d71f2ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose feature handling option for doctor's preferred features:\n",
      "1: Keep doctor's preferred features as non-mutable (always ON)\n",
      "2: Initialize doctor's preferred features as ON but allow mutation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: Fitness = 2.185289937662087, Accuracy = 0.717955326460481\n",
      "Generation 1: Fitness = 2.239749012985335, Accuracy = 0.7365979381443298\n",
      "Generation 2: Fitness = 2.2786872220064778, Accuracy = 0.7490120274914089\n",
      "Generation 3: Fitness = 2.280094051447716, Accuracy = 0.7510953608247424\n",
      "Generation 4: Fitness = 2.297304998266381, Accuracy = 0.7552835051546392\n",
      "Generation 5: Fitness = 2.297304998266381, Accuracy = 0.7552835051546392\n",
      "Converged at generation 5\n",
      "Final Best Solution (found at generation 4):\n",
      "  Selected Features: ['general grey matter', 'general csf', 'cerebellum', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'left cerebral white matter', 'left cerebral cortex', 'left caudate', 'left putamen', 'left pallidum', '4th ventricle', 'left hippocampus', 'left ventral DC', 'right cerebral white matter', 'right lateral ventricle', 'right inferior lateral ventricle', 'right cerebellum white matter', 'right thalamus', 'right putamen', 'right pallidum', 'right hippocampus', 'right amygdala', 'right accumbens area', 'right ventral DC', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiortemporal', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-lingual', 'ctx-lh-paracentral', 'ctx-lh-parsorbitalis', 'ctx-lh-parstriangularis', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-superiorparietal', 'ctx-lh-superiortemporal', 'ctx-lh-supramarginal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-lh-transversetemporal', 'ctx-lh-insula', 'ctx-rh-bankssts', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-entorhinal', 'ctx-rh-fusiform', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-pericalcarine', 'ctx-rh-postcentral', 'ctx-rh-posteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-superiortemporal', 'ctx-rh-supramarginal', 'ctx-rh-frontalpole']\n",
      "  Number of features selected: 57\n",
      "Best fitness values across generations: [2.185289937662087, 2.239749012985335, 2.2786872220064778, 2.280094051447716, 2.297304998266381, 2.297304998266381]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"resampled_data.csv\")  # Make sure the dataset is available in this path\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Doctor's preferred features (to be handled differently)\n",
    "preferred_features = [\n",
    "    'general grey matter', 'general csf', 'cerebellum', 'hippocampus+amygdala',\n",
    "    'left hippocampus', 'right hippocampus', 'ctx-lh-entorhinal', 'ctx-rh-entorhinal'\n",
    "]\n",
    "\n",
    "# Mapping feature names to their indices in the dataset\n",
    "preferred_feature_indices = [X.columns.get_loc(feature) for feature in preferred_features]\n",
    "\n",
    "# User input to decide the option (Option 1 or Option 2)\n",
    "print(\"Choose feature handling option for doctor's preferred features:\")\n",
    "print(\"1: Keep doctor's preferred features as non-mutable (always ON)\")\n",
    "print(\"2: Initialize doctor's preferred features as ON but allow mutation\")\n",
    "user_choice = int(input(\"Enter your choice (1 or 2): \"))\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features, preferred_feature_indices, user_choice):\n",
    "    # Creates a random individual (feature subset).\n",
    "    individual = [random.randint(0, 1) for _ in range(n_features)]\n",
    "    \n",
    "    if user_choice == 1:  # Option 1: Set preferred features to 1 and non-mutable\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    elif user_choice == 2:  # Option 2: Set preferred features to 1 initially, but allow mutation later\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def apply_lda(numeric_df, y):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df = pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "\n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    num_selected_features = len(selected_features)\n",
    "    \n",
    "    if num_selected_features == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy, precision, recall = apply_lda(pd.DataFrame(X_scaled), y)\n",
    "\n",
    "    # Calculate fitness with penalty for more features\n",
    "    fitness = accuracy + precision + recall + (1 / num_selected_features) \n",
    "    return fitness\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(individual, indpb=0.05, preferred_feature_indices=None, user_choice=None):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        \n",
    "        # Option 1: Keep doctor's preferred features as non-mutable\n",
    "        if user_choice == 1 and point in preferred_feature_indices:\n",
    "            return individual  # Skip mutation for preferred features\n",
    "\n",
    "        # Option 2: Allow mutation on all features, including the preferred ones\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    \n",
    "    return (individual,)\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features, preferred_feature_indices, user_choice) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i], preferred_feature_indices=preferred_feature_indices, user_choice=user_choice)\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Print generation information for the top individual\n",
    "        best_individual_current_gen = max(population, key=lambda ind: evaluate(ind, X, y))\n",
    "        \n",
    "        # Calculate X_scaled for the best individual\n",
    "        selected_features = [i for i, bit in enumerate(best_individual_current_gen) if bit == 1]\n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "        accuracy_top = apply_lda(pd.DataFrame(X_scaled), y)[0]  # Get accuracy for the top individual\n",
    "        print(f\"Generation {gen}: Fitness = {best_fitnesses[-1]}, Accuracy = {accuracy_top}\") \n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(f\"Best fitness values across generations: {best_fitnesses}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acdf93-b186-45d0-9dfb-dc4392263eee",
   "metadata": {},
   "source": [
    "## Now let's run the code on the UnderSampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9832467-761c-4887-8009-722634d4c6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose feature handling option for doctor's preferred features:\n",
      "1: Keep doctor's preferred features as non-mutable (always ON)\n",
      "2: Initialize doctor's preferred features as ON but allow mutation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: Fitness = 1.8187963113825658, Accuracy = 0.5989489489489489\n",
      "Generation 1: Fitness = 1.8879996770886358, Accuracy = 0.6214714714714715\n",
      "Generation 2: Fitness = 1.9015537764017059, Accuracy = 0.6270270270270271\n",
      "Generation 3: Fitness = 1.9015537764017059, Accuracy = 0.6270270270270271\n",
      "Converged at generation 3\n",
      "Final Best Solution (found at generation 2):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'total intracranial', 'left cerebral white matter', 'left lateral ventricle', 'left inferior lateral ventricle', 'left caudate', 'left putamen', '3rd ventricle', '4th ventricle', 'left hippocampus', 'left amygdala', 'right cerebral white matter', 'right lateral ventricle', 'right inferior lateral ventricle', 'right cerebellum white matter', 'right cerebellum cortex', 'right pallidum', 'right hippocampus', 'right amygdala', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-inferiortemporal', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-medialorbitofrontal', 'ctx-lh-parstriangularis', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-superiorparietal', 'ctx-lh-superiortemporal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-cuneus', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-isthmuscingulate', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-parahippocampal', 'ctx-rh-paracentral', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-parstriangularis', 'ctx-rh-posteriorcingulate', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-supramarginal', 'ctx-rh-frontalpole']\n",
      "  Number of features selected: 61\n",
      "Best fitness values across generations: [1.8187963113825658, 1.8879996770886358, 1.9015537764017059, 1.9015537764017059]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"undersampled_data.csv\")  # Make sure the dataset is available in this path\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Doctor's preferred features (to be handled differently)\n",
    "preferred_features = [\n",
    "    'general grey matter', 'general csf', 'cerebellum', 'hippocampus+amygdala',\n",
    "    'left hippocampus', 'right hippocampus', 'ctx-lh-entorhinal', 'ctx-rh-entorhinal'\n",
    "]\n",
    "\n",
    "# Mapping feature names to their indices in the dataset\n",
    "preferred_feature_indices = [X.columns.get_loc(feature) for feature in preferred_features]\n",
    "\n",
    "# User input to decide the option (Option 1 or Option 2)\n",
    "print(\"Choose feature handling option for doctor's preferred features:\")\n",
    "print(\"1: Keep doctor's preferred features as non-mutable (always ON)\")\n",
    "print(\"2: Initialize doctor's preferred features as ON but allow mutation\")\n",
    "user_choice = int(input(\"Enter your choice (1 or 2): \"))\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features, preferred_feature_indices, user_choice):\n",
    "    # Creates a random individual (feature subset).\n",
    "    individual = [random.randint(0, 1) for _ in range(n_features)]\n",
    "    \n",
    "    if user_choice == 1:  # Option 1: Set preferred features to 1 and non-mutable\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    elif user_choice == 2:  # Option 2: Set preferred features to 1 initially, but allow mutation later\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def apply_lda(numeric_df, y):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df = pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "\n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    num_selected_features = len(selected_features)\n",
    "    \n",
    "    if num_selected_features == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy, precision, recall = apply_lda(pd.DataFrame(X_scaled), y)\n",
    "\n",
    "    # Calculate fitness with penalty for more features\n",
    "    fitness = accuracy + precision + recall + (1 / num_selected_features) \n",
    "    return fitness\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(individual, indpb=0.05, preferred_feature_indices=None, user_choice=None):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        \n",
    "        # Option 1: Keep doctor's preferred features as non-mutable\n",
    "        if user_choice == 1 and point in preferred_feature_indices:\n",
    "            return individual  # Skip mutation for preferred features\n",
    "\n",
    "        # Option 2: Allow mutation on all features, including the preferred ones\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    \n",
    "    return (individual,)\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features, preferred_feature_indices, user_choice) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i], preferred_feature_indices=preferred_feature_indices, user_choice=user_choice)\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Print generation information for the top individual\n",
    "        best_individual_current_gen = max(population, key=lambda ind: evaluate(ind, X, y))\n",
    "        \n",
    "        # Calculate X_scaled for the best individual\n",
    "        selected_features = [i for i, bit in enumerate(best_individual_current_gen) if bit == 1]\n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "        accuracy_top = apply_lda(pd.DataFrame(X_scaled), y)[0]  # Get accuracy for the top individual\n",
    "        print(f\"Generation {gen}: Fitness = {best_fitnesses[-1]}, Accuracy = {accuracy_top}\") \n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(f\"Best fitness values across generations: {best_fitnesses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c59521-9d05-41fe-870a-bcc137d4b25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose feature handling option for doctor's preferred features:\n",
      "1: Keep doctor's preferred features as non-mutable (always ON)\n",
      "2: Initialize doctor's preferred features as ON but allow mutation\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: Fitness = 1.8187963113825658, Accuracy = 0.5989489489489489\n",
      "Generation 1: Fitness = 1.8879996770886358, Accuracy = 0.6214714714714715\n",
      "Generation 2: Fitness = 1.9015537764017059, Accuracy = 0.6270270270270271\n",
      "Generation 3: Fitness = 1.9015537764017059, Accuracy = 0.6270270270270271\n",
      "Converged at generation 3\n",
      "Final Best Solution (found at generation 2):\n",
      "  Selected Features: ['general white matter', 'general grey matter', 'general csf', 'cerebellum', 'brainstem', 'putamen+pallidum', 'hippocampus+amygdala', 'total intracranial', 'left cerebral white matter', 'left lateral ventricle', 'left inferior lateral ventricle', 'left caudate', 'left putamen', '3rd ventricle', '4th ventricle', 'left hippocampus', 'left amygdala', 'right cerebral white matter', 'right lateral ventricle', 'right inferior lateral ventricle', 'right cerebellum white matter', 'right cerebellum cortex', 'right pallidum', 'right hippocampus', 'right amygdala', 'ctx-lh-bankssts', 'ctx-lh-caudalanteriorcingulate', 'ctx-lh-entorhinal', 'ctx-lh-fusiform', 'ctx-lh-inferiorparietal', 'ctx-lh-inferiortemporal', 'ctx-lh-isthmuscingulate', 'ctx-lh-lateralorbitofrontal', 'ctx-lh-medialorbitofrontal', 'ctx-lh-parstriangularis', 'ctx-lh-posteriorcingulate', 'ctx-lh-precentral', 'ctx-lh-precuneus', 'ctx-lh-superiorparietal', 'ctx-lh-superiortemporal', 'ctx-lh-frontalpole', 'ctx-lh-temporalpole', 'ctx-rh-caudalmiddlefrontal', 'ctx-rh-cuneus', 'ctx-rh-entorhinal', 'ctx-rh-inferiorparietal', 'ctx-rh-inferiortemporal', 'ctx-rh-isthmuscingulate', 'ctx-rh-lateraloccipital', 'ctx-rh-lateralorbitofrontal', 'ctx-rh-parahippocampal', 'ctx-rh-paracentral', 'ctx-rh-parsopercularis', 'ctx-rh-parsorbitalis', 'ctx-rh-parstriangularis', 'ctx-rh-posteriorcingulate', 'ctx-rh-precuneus', 'ctx-rh-rostralanteriorcingulate', 'ctx-rh-rostralmiddlefrontal', 'ctx-rh-supramarginal', 'ctx-rh-frontalpole']\n",
      "  Number of features selected: 61\n",
      "Best fitness values across generations: [1.8187963113825658, 1.8879996770886358, 1.9015537764017059, 1.9015537764017059]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv(\"undersampled_data.csv\")  # Make sure the dataset is available in this path\n",
    "y = data['decision'].astype(bool)\n",
    "X = data.drop(columns=['Subject', 'decision', 'neuropsych_score'], axis=1)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# Doctor's preferred features (to be handled differently)\n",
    "preferred_features = [\n",
    "    'general grey matter', 'general csf', 'cerebellum', 'hippocampus+amygdala',\n",
    "    'left hippocampus', 'right hippocampus', 'ctx-lh-entorhinal', 'ctx-rh-entorhinal'\n",
    "]\n",
    "\n",
    "# Mapping feature names to their indices in the dataset\n",
    "preferred_feature_indices = [X.columns.get_loc(feature) for feature in preferred_features]\n",
    "\n",
    "# User input to decide the option (Option 1 or Option 2)\n",
    "print(\"Choose feature handling option for doctor's preferred features:\")\n",
    "print(\"1: Keep doctor's preferred features as non-mutable (always ON)\")\n",
    "print(\"2: Initialize doctor's preferred features as ON but allow mutation\")\n",
    "user_choice = int(input(\"Enter your choice (1 or 2): \"))\n",
    "\n",
    "# Genetic Algorithm Functions From Scratch\n",
    "def create_individual(n_features, preferred_feature_indices, user_choice):\n",
    "    # Creates a random individual (feature subset).\n",
    "    individual = [random.randint(0, 1) for _ in range(n_features)]\n",
    "    \n",
    "    if user_choice == 1:  # Option 1: Set preferred features to 1 and non-mutable\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    elif user_choice == 2:  # Option 2: Set preferred features to 1 initially, but allow mutation later\n",
    "        for idx in preferred_feature_indices:\n",
    "            individual[idx] = 1\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def apply_lda(numeric_df, y):\n",
    "    n_components = 1\n",
    "    lda = LDA(n_components=n_components)\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "\n",
    "    for train_index, test_index in skf.split(numeric_df, y):\n",
    "        X_train_fold, X_test_fold = numeric_df.iloc[train_index], numeric_df.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_lda = lda.fit_transform(X_train_fold, y_train_fold)\n",
    "        lda_df = pd.DataFrame(data=X_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        classifier.fit(lda_df, y_train_fold)\n",
    "\n",
    "        X_test_lda = lda.transform(X_test_fold)\n",
    "        X_test_df = pd.DataFrame(data=X_test_lda, columns=[f'LD{i+1}' for i in range(n_components)])\n",
    "        y_pred_fold = classifier.predict(X_test_df)\n",
    "\n",
    "        precision_scores.append(precision_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        recall_scores.append(recall_score(y_test_fold, y_pred_fold, average='macro'))\n",
    "        accuracy_scores.append(classifier.score(X_test_df, y_test_fold))\n",
    "\n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores)\n",
    "\n",
    "def evaluate(individual, X, y):\n",
    "    # Evaluates the fitness of an individual.\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n",
    "    num_selected_features = len(selected_features)\n",
    "    \n",
    "    if num_selected_features == 0:\n",
    "        return 0  # Avoid selecting no features\n",
    "\n",
    "    X_selected = X.iloc[:, selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "    # Apply LDA using the apply_lda function\n",
    "    accuracy, precision, recall = apply_lda(pd.DataFrame(X_scaled), y)\n",
    "\n",
    "    # Calculate fitness with penalty for more features\n",
    "    fitness = accuracy + precision + recall + (1 / num_selected_features) \n",
    "    return fitness\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    # Performs crossover at a random point.\n",
    "    point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = parent1[:point] + parent2[point:]\n",
    "    child2 = parent2[:point] + parent1[point:]\n",
    "    return child1, child2\n",
    "\n",
    "def mutation(individual, indpb=0.05, preferred_feature_indices=None, user_choice=None):\n",
    "    # Performs flip-bit mutation at a single random point.\n",
    "    if random.random() < indpb:  # Check if mutation should occur\n",
    "        point = random.randint(0, len(individual) - 1)  # Select a random point\n",
    "        \n",
    "        # Option 1: Keep doctor's preferred features as non-mutable\n",
    "        if user_choice == 1 and point in preferred_feature_indices:\n",
    "            return individual  # Skip mutation for preferred features\n",
    "\n",
    "        # Option 2: Allow mutation on all features, including the preferred ones\n",
    "        individual[point] = 1 - individual[point]  # Flip the bit at the selected point\n",
    "    \n",
    "    return (individual,)\n",
    "\n",
    "# Main Genetic Algorithm Call Function\n",
    "def main(X, y, pop_size=100, ngen=10, cxpb=0.5, mutpb=0.2, top_percent=0.10, tolerance=1e-4):\n",
    "    random.seed(42)\n",
    "\n",
    "    # Initialize population\n",
    "    population = [create_individual(n_features, preferred_feature_indices, user_choice) for _ in range(pop_size)]\n",
    "\n",
    "    # Evaluate initial population\n",
    "    fitnesses = [evaluate(ind, X, y) for ind in population]\n",
    "\n",
    "    # Store best individuals, their generation, and their fitness values\n",
    "    best_individuals = []\n",
    "    best_generations = []\n",
    "    best_fitnesses = []\n",
    "\n",
    "    for gen in range(ngen):\n",
    "        # Selection (Select top individuals directly)\n",
    "        num_top = int(pop_size * top_percent)\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        offspring = [population[i] for i in top_indices]\n",
    "\n",
    "        # Crossover to create the rest of the offspring\n",
    "        num_crossovers = pop_size - num_top  # Number of crossovers needed\n",
    "        for _ in range(num_crossovers // 2):  # // 2 because each crossover creates 2 offspring\n",
    "            parent1, parent2 = random.sample(offspring, 2)  # Select parents from the top individuals\n",
    "            child1, child2 = crossover(parent1, parent2)\n",
    "            offspring.extend([child1, child2])\n",
    "\n",
    "        # Mutation\n",
    "        for i in range(num_top, len(offspring)):  # Mutate only the new offspring\n",
    "            if random.random() < mutpb:\n",
    "                offspring[i], = mutation(offspring[i], preferred_feature_indices=preferred_feature_indices, user_choice=user_choice)\n",
    "\n",
    "        # Evaluate offspring\n",
    "        fitnesses = [evaluate(ind, X, y) for ind in offspring]\n",
    "\n",
    "        # Replace population with offspring (Elitism: Keep top individuals)\n",
    "        population[:] = offspring  \n",
    "\n",
    "        # Store the top individuals and their generation\n",
    "        top_indices = np.argsort(fitnesses)[-num_top:]\n",
    "        for i in top_indices:\n",
    "            best_individuals.append(population[i])\n",
    "            best_generations.append(gen)  # Store the generation\n",
    "\n",
    "        # Store fitness of the best individual in the current generation\n",
    "        best_fitnesses.append(max(fitnesses))\n",
    "\n",
    "        # Print generation information for the top individual\n",
    "        best_individual_current_gen = max(population, key=lambda ind: evaluate(ind, X, y))\n",
    "        \n",
    "        # Calculate X_scaled for the best individual\n",
    "        selected_features = [i for i, bit in enumerate(best_individual_current_gen) if bit == 1]\n",
    "        X_selected = X.iloc[:, selected_features]\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "        accuracy_top = apply_lda(pd.DataFrame(X_scaled), y)[0]  # Get accuracy for the top individual\n",
    "        print(f\"Generation {gen}: Fitness = {best_fitnesses[-1]}, Accuracy = {accuracy_top}\") \n",
    "\n",
    "        # Convergence check\n",
    "        if gen > 1 and abs(best_fitnesses[-1] - best_fitnesses[-2]) < tolerance:\n",
    "            print(f\"Converged at generation {gen}\")\n",
    "            break\n",
    "\n",
    "    return best_individuals, best_generations, best_fitnesses  # Return all three lists\n",
    "\n",
    "# Run the GA\n",
    "best_individuals, best_generations, best_fitnesses = main(X, y, pop_size=100, top_percent=0.10)  # Use top 10%\n",
    "\n",
    "# Print the final best solution and its generation\n",
    "best_individual = max(best_individuals, key=lambda ind: evaluate(ind, X, y))\n",
    "best_index = best_individuals.index(best_individual)\n",
    "best_generation = best_generations[best_index]\n",
    "\n",
    "selected_features_lda = [i for i, bit in enumerate(best_individual) if bit == 1]\n",
    "print(f\"Final Best Solution (found at generation {best_generation}):\")\n",
    "print(f\"  Selected Features: {X.columns[selected_features_lda].tolist()}\")\n",
    "print(f\"  Number of features selected: {len(selected_features_lda)}\")\n",
    "\n",
    "# Print fitness values across generations\n",
    "print(f\"Best fitness values across generations: {best_fitnesses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9262133-bdf2-4d11-bea2-30db0e8f4f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
